## 这个项目旨在于研究训练数据精度，Batch Size，以及有无CBAM注意力对于收敛速率的影响。

### 实验内容

使用ResNet-18作为基础网络，训练CIFAR-10数据集，分别使用FP32，TF32，BF16精度，Batch Size为16，32，64，有无CBAM注意力模块，训练10个Epoch，观察Loss的收敛速率。

### 结论

* TF32能够取得与FP64相当的收敛速率，而BF16则会有明显的收敛速率下降，且收敛困难
* 相同计算量下（即相同Epoch），Batch Size对于收敛速率无明显影响，但是大Batch Size可以减少Loss波动
* CBAM注意力模块可以加速收敛速率，且在训练的200-1500次迭代就可以观察出，在后续的训练中，Loss差的绝对值几乎没有改变
* CBMA最好使用在网络的更多层次，只有一层效果不很好